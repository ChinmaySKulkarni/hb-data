# hb-data
## Assumptions:

- Data is valid and follows mentioned schema
- Current solution does not scale if we query regularly. It would be better in that case to read the parquet files from S3
and re-write them in S3 but with partitioning in say a Hive/Iceberg table. Partitioning would depend on query patterns and file sizes generated by workers
- For scaling, we could horizontally scale by adding worker nodes. We could also vertically scale by getting beefier EC2 instances (in case we are managing the spark workers ourselves) by example getting workers with higher memory. This can be beneficial for caching dataframes
- If we wanted to build a service, it would be beneficial to make a table like mentioned above and optimize queries to be along partitions and vice versa. A REST service could be the endpoint for clients and the service could issue prepared queries to the backing datastore
- For productionizing, we would probably add load balancing at some point and have region local REST server and EC2 instance for ex.
