{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60566d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# <nbformat>3.0</nbformat>\n",
    "# <markdowncell>\n",
    "# Markdown cells are embedded in comments, so the file is a valid `python` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "import great_expectations as ge\n",
    "import json\n",
    "\n",
    "# Use s3a for accessing the parquet file in case it exceeds 5GB and for perf enhancements over s3n and native s3\n",
    "S3_URL = \"s3a://hb-data-eng-test/events/part-00000-d3813b59-7d2f-429b-80ed-76bddb41e9bb-c000.snappy.parquet\"\n",
    "HADOOP_CONF_FILE_LOCATION = \"conf/hadoop-conf.yml\"\n",
    "EXPECTATIONS_DIR = \"expectations\"\n",
    "RUN_WITH_VALIDATION = False\n",
    "# run with 4 cores\n",
    "SPARK_MASTER_URL = \"local[4]\"\n",
    "SPARK_APP_NAME = \"PySpark HB Application Processor\"\n",
    "SPARK_SESSION = None\n",
    "\n",
    "\n",
    "def find_top_n_locations(n=20):\n",
    "    # Skip null locations which are probably not too useful anyways\n",
    "    df = SPARK_SESSION.sql(\n",
    "        \"SELECT LOCATION_ID, COUNT(*) AS TOTAL_COUNT FROM T \" +\n",
    "        \"WHERE LOCATION_ID IS NOT NULL \"\n",
    "        \"GROUP BY LOCATION_ID ORDER BY TOTAL_COUNT DESC LIMIT \" + str(n))\n",
    "\n",
    "    if RUN_WITH_VALIDATION:\n",
    "        print(\"Validating dataframe for task b) via Great Expectations\")\n",
    "        df_ge = ge.dataset.SparkDFDataset(df)\n",
    "        df_ge.expect_column_to_exist(\"LOCATION_ID\")\n",
    "        df_ge.expect_column_to_exist(\"TOTAL_COUNT\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"TOTAL_COUNT\", \"IntegerType\")\n",
    "        df_ge.expect_table_row_count_to_equal(20)\n",
    "        df_ge.expect_column_values_to_be_unique(\"LOCATION_ID\")\n",
    "        df_ge.expect_column_values_to_not_be_null(\"LOCATION_ID\")\n",
    "        df_ge.expect_column_values_to_be_decreasing(\"TOTAL_COUNT\")\n",
    "        __persist_expectation(\"task_b_find_top_n_locations\", df_ge)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_timesheets_active_first_week():\n",
    "    df = SPARK_SESSION.sql(\n",
    "        \"SELECT COMPANY_ID, IF(TOTAL_INSTANCES > 0, TRUE, FALSE) AS TIMESHEETS_ACTIVE_FIRST_WEEK \" +\n",
    "        \"FROM (SELECT COMPANY_ID, SUM(IF(((PRODUCT_AREA='Timesheets' OR PRODUCT_AREA='timesheets') AND \" +\n",
    "        \"(CAST(DATE_FORMAT(CREATED_AT, 'd') AS INT) <= 7)), 1, 0)) AS TOTAL_INSTANCES FROM T GROUP BY COMPANY_ID)\")\n",
    "\n",
    "    if RUN_WITH_VALIDATION:\n",
    "        print(\"Validating dataframe for task c) via Great Expectations\")\n",
    "        df_ge = ge.dataset.SparkDFDataset(df)\n",
    "        df_ge.expect_column_to_exist(\"COMPANY_ID\")\n",
    "        df_ge.expect_column_to_exist(\"TIMESHEETS_ACTIVE_FIRST_WEEK\")\n",
    "        df_ge.expect_column_values_to_be_unique(\"COMPANY_ID\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"TIMESHEETS_ACTIVE_FIRST_WEEK\", \"BooleanType\")\n",
    "        __persist_expectation(\"task_c_find_timesheets_active_first_week\", df_ge)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_events_grouped_by_location_device():\n",
    "    df = SPARK_SESSION.sql(\n",
    "        \"SELECT LOCATION_ID, DEVICE_OS, COUNT(*) AS TOTAL_EVENTS FROM T \" +\n",
    "        \"GROUP BY LOCATION_ID, DEVICE_OS HAVING TOTAL_EVENTS > 0\")\n",
    "\n",
    "    if RUN_WITH_VALIDATION:\n",
    "        print(\"Validating dataframe for task d) via Great Expectations\")\n",
    "        df_ge = ge.dataset.SparkDFDataset(df)\n",
    "        df_ge.expect_column_to_exist(\"LOCATION_ID\")\n",
    "        df_ge.expect_column_to_exist(\"DEVICE_OS\")\n",
    "        df_ge.expect_column_to_exist(\"TOTAL_EVENTS\")\n",
    "        df_ge.expect_compound_columns_to_be_unique([\"LOCATION_ID\", \"DEVICE_OS\"])\n",
    "        df_ge.expect_column_values_to_be_of_type(\"TOTAL_EVENTS\", \"IntegerType\")\n",
    "        df_ge.expect_column_values_to_be_between(\"TOTAL_EVENTS\", 0, None, True)\n",
    "        __persist_expectation(\"task_d_find_events_grouped_by_location_device\", df_ge)\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_configs(conf_location):\n",
    "    with open(conf_location, 'r') as conf_fd:\n",
    "        return yaml.safe_load(conf_fd)\n",
    "\n",
    "\n",
    "def init_dataframe():\n",
    "    df = SPARK_SESSION.read.parquet(S3_URL)\n",
    "    df.createOrReplaceTempView('T')\n",
    "\n",
    "    if RUN_WITH_VALIDATION:\n",
    "        print(\"Validating dataframe for task a) via Great Expectations\")\n",
    "        df_ge = ge.dataset.SparkDFDataset(df)\n",
    "        # validate expected schema\n",
    "        df_ge.expect_column_to_exist(\"COMPANY_ID\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"COMPANY_ID\", \"IntegerType\")\n",
    "        df_ge.expect_column_to_exist(\"LOCATION_ID\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"LOCATION_ID\", \"IntegerType\")\n",
    "        df_ge.expect_column_to_exist(\"USER_ID\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"USER_ID\", \"IntegerType\")\n",
    "        df_ge.expect_column_to_exist(\"CREATED_AT\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"CREATED_AT\", \"TimestampType\")\n",
    "        df_ge.expect_column_to_exist(\"PRODUCT_AREA\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"PRODUCT_AREA\", \"StringType\")\n",
    "        df_ge.expect_column_to_exist(\"EVENT_ACTION\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"EVENT_ACTION\", \"StringType\")\n",
    "        df_ge.expect_column_to_exist(\"DEVICE_OS\")\n",
    "        df_ge.expect_column_values_to_be_of_type(\"DEVICE_OS\", \"StringType\")\n",
    "        __persist_expectation(\"task_a_init_dataframe\", df_ge)\n",
    "\n",
    "\n",
    "# Note: passing in global variables to make this easily testable via dependency injection\n",
    "def get_or_generate_spark_session(spark_session_builder, conf_map, spark_master_url, spark_app_name):\n",
    "    spark_session_builder = spark_session_builder \\\n",
    "        .master(spark_master_url) \\\n",
    "        .appName(spark_app_name)\n",
    "    keys = list(conf_map)\n",
    "    for k in keys:\n",
    "        spark_session_builder.config(k, conf_map[k])\n",
    "    return spark_session_builder.getOrCreate()\n",
    "\n",
    "\n",
    "# persist expectations for each task. We can use these expectations to\n",
    "# validate our dataframes against new data in the future\n",
    "def __persist_expectation(task_name, df_ge):\n",
    "    with open(EXPECTATIONS_DIR + \"/expectation_\" + task_name + \".json\", \"w\") as fd:\n",
    "        fd.write(json.dumps(df_ge.get_expectation_suite().to_json_dict()))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Make these global so we can interact with it in Jupyter\n",
    "    global SPARK_SESSION\n",
    "    print(\"******* Task a) Reading input Parquet data from S3 into a Dataframe *******\")\n",
    "    conf_map = parse_configs(HADOOP_CONF_FILE_LOCATION)\n",
    "    SPARK_SESSION = get_or_generate_spark_session(SparkSession.builder, conf_map, SPARK_MASTER_URL, SPARK_APP_NAME)\n",
    "    init_dataframe()\n",
    "\n",
    "    print(\"******* Task b) Retrieving the top 20 events grouped by location *******\")\n",
    "    df_b = find_top_n_locations(20)\n",
    "    # pass in `20` in show as well, in case predicate push-down of limit is not supported\n",
    "    df_b.show(20, False, False)\n",
    "\n",
    "    print(\"******* Task c) Retrieving if there were active timesheets in the first week for each company *******\")\n",
    "    df_c = find_timesheets_active_first_week()\n",
    "    df_c.show(50, False, False)\n",
    "\n",
    "    print(\"******* Task d) Retrieving events grouped by location_id and device_os *******\")\n",
    "    df_d = find_events_grouped_by_location_device()\n",
    "    df_d.show(50, False, False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# <rawcell>\n",
    "# Raw cell contents are not formatted as markdown\n",
    "# <markdowncell>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
