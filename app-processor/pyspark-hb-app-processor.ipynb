{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <nbformat>3.0</nbformat>\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# Markdown cells are embedded in comments, so the file is a valid `python` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8429715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "\n",
    "# Use s3a for accessing the parquet file in case it exceeds 5GB and for perf enhancements over s3n and native s3\n",
    "S3_URL = \"s3a://hb-data-eng-test/events/part-00000-d3813b59-7d2f-429b-80ed-76bddb41e9bb-c000.snappy.parquet\"\n",
    "SPARK_CONF_FILE_LOCATION = \"./conf/spark-conf.yml\"\n",
    "# run with 4 cores\n",
    "SPARK_MASTER_URL = \"local[4]\"\n",
    "SPARK_APP_NAME = \"PySpark HB Application Processor\"\n",
    "\n",
    "\n",
    "def find_top_n_locations(spark_session, url, n=20):\n",
    "    df = spark_session.read.parquet(url)\n",
    "    df.printSchema()\n",
    "\n",
    "\n",
    "def parse_configs(conf_location):\n",
    "    with open(conf_location, 'r') as conf_fd:\n",
    "        return yaml.safe_load(conf_fd)\n",
    "\n",
    "\n",
    "# TODO: Note: passing in global variables to make this testable potentially.\n",
    "def get_or_generate_spark_session(conf_location, spark_master_url, spark_app_name):\n",
    "    conf_map = parse_configs(conf_location)\n",
    "    spark_session_builder = SparkSession.builder \\\n",
    "        .master(spark_master_url) \\\n",
    "        .appName(spark_app_name)\n",
    "    keys = list(conf_map)\n",
    "    for k in keys:\n",
    "        spark_session_builder.config(k, conf_map[k])\n",
    "    return spark_session_builder.getOrCreate()\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark_session = get_or_generate_spark_session(SPARK_CONF_FILE_LOCATION, SPARK_MASTER_URL, SPARK_APP_NAME)\n",
    "    find_top_n_locations(spark_session, S3_URL, 20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# <rawcell>\n",
    "\n",
    "# Raw cell contents are not formatted as markdown\n",
    "# <markdowncell>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
